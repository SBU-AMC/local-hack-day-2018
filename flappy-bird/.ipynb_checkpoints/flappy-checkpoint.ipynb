{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ple.games.flappybird  import FlappyBird\n",
    "from ple import PLE\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import random\n",
    "ch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = FlappyBird()\n",
    "\n",
    "p = PLE(game, fps=30, display_screen=True, force_fps=False)\n",
    "p.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'up': 119}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game.step(119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_frames = 1000\n",
    "reward = 0.0\n",
    "\n",
    "for f in range(nb_frames):\n",
    "    if game.game_over(): #check if the game is over\n",
    "        game.reset()\n",
    "#         break\n",
    "\n",
    "    obs = game.getScreenRGB()\n",
    "#     state = game.getGameState()\n",
    "#     break\n",
    "#     action = game.(119)\n",
    "    reward = p.act(119)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (128, 128, 3)\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    \n",
    "    def __init__(self, max_mem_size):\n",
    "        \n",
    "        \n",
    "        self.counter = 0\n",
    "        \n",
    "        self.list = []\n",
    "        \n",
    "        self.max_size = max_mem_size\n",
    "        \n",
    "        \n",
    "    def add(self, obj):\n",
    "        \n",
    "        if self.counter >= self.max_size :\n",
    "            \n",
    "            self.list[self.counter % self.max_size] = obj\n",
    "            \n",
    "        \n",
    "        else :\n",
    "            \n",
    "            self.list.append(obj)\n",
    "            \n",
    "        \n",
    "        self.counter += 1\n",
    "    \n",
    "    def get(self, batch_size):\n",
    "        \n",
    "        ind = np.arange(len(self.list))\n",
    "        ind = np.random.choice(ind, batch_size, replace=False)\n",
    "        return [self.list[i] for i in ind]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooria_act(action):\n",
    "    \n",
    "#     print(action)\n",
    "    \n",
    "    ind = np.argmax(action)\n",
    "    \n",
    "    p_a = [119, None, 119, None]\n",
    "    \n",
    "    if ind == 0 :\n",
    "        p.act(p_a[ind])\n",
    "        p.act(p_a[ind])\n",
    "\n",
    "    if ind == 1 :\n",
    "        p.act(p_a[ind])\n",
    "        p.act(p_a[ind])\n",
    "    if ind == 3 :\n",
    "        p.act(p_a[ind])\n",
    "    else :\n",
    "        p.act(p_a[ind])\n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddown = [0, 1, 0, 0]\n",
    "dup = [1, 0, 0, 0]\n",
    "up = [0, 0, 1, 0]\n",
    "down = [0, 0, 0, 1]\n",
    "\n",
    "possible_actions = [dup, ddown, up, down]\n",
    "total_episodes = 500\n",
    "max_steps = 125\n",
    "batch_size = 128\n",
    "explore_start = 1.0\n",
    "explore_stop = 0.01\n",
    "decay_rate = 1e-4\n",
    "num_seq = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "memory_size = 1000\n",
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_picture(game, before = None, arr = None):\n",
    "    if arr is None:\n",
    "        x = game.getScreenRGB().astype(np.uint8)\n",
    "        x = Image.fromarray(x).resize([128, 128])\n",
    "        x = np.array(x).astype(np.float32)\n",
    "    else : \n",
    "        x = arr.astype(np.float32)\n",
    "    x = x / 255.0\n",
    "    if before is None:\n",
    "        return np.concatenate([x for i in range(num_seq)], axis=-1)\n",
    "    \n",
    "    else :\n",
    "        res = before.copy()\n",
    "        res [:,:,:-3] = before[:,:,3:]\n",
    "        res[:,:,-3:]=x\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, shape):\n",
    "        self.input_ = tf.placeholder(shape = [None, *shape], dtype=tf.float32)\n",
    "        self.actions_ = tf.placeholder(shape=[None, len(possible_actions)], dtype=tf.float32)\n",
    "        self.target_Q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.conv1 = tf.layers.conv2d(self.input_, 32,\n",
    "                                      kernel_size=(5,5), strides=(2,2),\n",
    "                                      padding=\"SAME\", activation=None, name=\"conv1\",\n",
    "                                      kernel_initializer= tf.glorot_uniform_initializer(dtype=tf.float16))\n",
    "        self.batch_norm1 = tf.layers.batch_normalization(self.conv1, training=True, name=\"ba1\")\n",
    "        self.conv1_out = tf.nn.elu(self.batch_norm1, name=\"out1\")\n",
    "        \n",
    "        \n",
    "        self.conv2 = tf.layers.conv2d(self.conv1_out, 64, [5,5],\n",
    "                                      strides=(2,2), padding=\"SAME\", activation=None, name=\"conv2\",\n",
    "                                      kernel_initializer= tf.glorot_uniform_initializer(dtype=tf.float16))\n",
    "        self.batch_norm2 = tf.layers.batch_normalization(self.conv2, training=True, name=\"ba2\")\n",
    "        self.conv2_out = tf.nn.elu(self.batch_norm2, name = \"out2\")\n",
    "        \n",
    "        self.conv3 = tf.layers.conv2d(self.conv2_out, 128, [5,5], [2,2], activation=None, name=\"conv3\", padding=\"SAME\",\n",
    "                                      kernel_initializer= tf.glorot_uniform_initializer(dtype=tf.float16))\n",
    "        self.batch_norm3 = tf.layers.batch_normalization(self.conv3, training=True, name=\"ba3\")\n",
    "        self.conv3_out = tf.nn.elu(self.batch_norm3, name=\"out3\")\n",
    "        \n",
    "        \n",
    "        self.conv4 = tf.layers.conv2d(self.conv3_out, 128, [5,5], [2,2], activation=None, name=\"conv4\", padding=\"SAME\",\n",
    "                                      kernel_initializer= tf.glorot_uniform_initializer(dtype=tf.float16))\n",
    "        self.batch_norm4 = tf.layers.batch_normalization(self.conv4, training=True, name=\"ba4\")\n",
    "        self.conv4_out = tf.nn.elu(self.batch_norm4, name=\"out4\")\n",
    "        \n",
    "        \n",
    "        self.flat = tf.layers.flatten(self.conv4_out)\n",
    "        \n",
    "        self.fc1 = tf.layers.dense(self.flat, 1000, activation=tf.nn.relu, name=\"fc1\",\n",
    "                                      kernel_initializer= tf.glorot_uniform_initializer(dtype=tf.float16))\n",
    "        self.fc2 = tf.layers.dense(self.fc1, 100, activation=tf.nn.relu, name=\"fc2\",\n",
    "                                      kernel_initializer= tf.glorot_uniform_initializer(dtype=tf.float16))\n",
    "        self.out = tf.layers.dense(self.fc2, len(possible_actions), activation=None, name=\"fc3\",\n",
    "                                      kernel_initializer= tf.glorot_uniform_initializer(dtype=tf.float16))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.out, self.actions_), axis=1)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(self.Q - self.target_Q))\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=2e-4).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "nn = NeuralNetwork([*shape[:-1],shape[-1] * num_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.reset()\n",
    "reward = 0\n",
    "for i in range(int(batch_size)):\n",
    "    if i == 0 :\n",
    "        state = get_picture(game)\n",
    "    ac = random.choice(possible_actions)\n",
    "    pooria_act(ac)\n",
    "    done = game.game_over()\n",
    "    if done :\n",
    "        \n",
    "        next_state = get_picture(game, state, np.zeros([*state.shape[:-1],3]))\n",
    "        mem.add((state, ac, next_state, reward, done))\n",
    "        reward = 0\n",
    "        game.reset()\n",
    "        state = get_picture(game)\n",
    "        \n",
    "    else:\n",
    "        reward += 1\n",
    "\n",
    "        next_state = get_picture(game, state)\n",
    "        mem.add((state, ac, next_state, reward, done))\n",
    "        state = next_state\n",
    "#     if i % 1000 == 0 :\n",
    "#         print(sys.getsizeof(mem.mem))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    \n",
    "    rand = np.random.rand()\n",
    "    \n",
    "    \n",
    "    explore_prob = explore_stop + (explore_start - explore_stop) * np.exp(-decay_step * decay_rate)\n",
    "    \n",
    "    \n",
    "    if explore_prob > rand :\n",
    "        \n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else : \n",
    "        Qs = sess.run(nn.out, feed_dict={nn.input_:state})\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "    return action, explore_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9968370633976026 496 32 1942.75\n",
      "Model Saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-7b809d0003e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mQs_next_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnext_states_mb\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mtarget_Qs_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if training == True :\n",
    "    with tf.Session() as sess :\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        decay_step = 0\n",
    "        \n",
    "        game.reset()\n",
    "        \n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            \n",
    "            step = 0\n",
    "            \n",
    "            episode_reward = []\n",
    "            game.reset()\n",
    "            \n",
    "            state = get_picture(game)\n",
    "            st = 0\n",
    "            \n",
    "            reward = 0\n",
    "            \n",
    "            while step < max_steps :\n",
    "                \n",
    "                step += 1 \n",
    "                decay_step += 1\n",
    "                st += 1\n",
    "                \n",
    "                action, explore_prob = predict_next(explore_start, explore_stop, decay_rate, \n",
    "                                                    decay_step, state.reshape((1,)+state.shape), possible_actions)\n",
    "                \n",
    "                pooria_act(action)\n",
    "                \n",
    "                \n",
    "                done = game.game_over()\n",
    "                \n",
    "                episode_reward.append(reward)\n",
    "                \n",
    "\n",
    "                if done : \n",
    "                    \n",
    "                    \n",
    "                    next_state = get_picture(game, state, np.zeros(shape))\n",
    "                    #the next line is instead of a break\n",
    "                    step = max_steps\n",
    "                    mem.add((state, action, next_state, reward, done))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                else :\n",
    "                    \n",
    "                    reward += 1\n",
    "                    \n",
    "                    next_state = get_picture(game, state)\n",
    "                    mem.add((state, action, next_state, reward, done))\n",
    "                    state = next_state\n",
    "                \n",
    "                \n",
    "                batch = mem.get(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[3] for each in batch]) \n",
    "                next_states_mb = np.array([each[2] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "                \n",
    "                \n",
    "                Qs_next_state = sess.run(nn.out, feed_dict={nn.input_:next_states_mb})\n",
    "                \n",
    "                target_Qs_batch = []\n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    \n",
    "                    t = dones_mb[i]\n",
    "                    \n",
    "                    \n",
    "                    if t : \n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                    else :\n",
    "                        target = rewards_mb[i] + gamma*np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                \n",
    "                target_Qs_batch = np.array(target_Qs_batch)\n",
    "                feed_dict = {nn.input_:states_mb, nn.target_Q:target_Qs_batch, nn.actions_:actions_mb}\n",
    "                sess.run(nn.optimizer, feed_dict)\n",
    "                \n",
    "            \n",
    "            loss = nn.loss.eval(session=sess, feed_dict=feed_dict)\n",
    "            print(episode, explore_prob, np.sum(episode_reward),st, loss)\n",
    "            \n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
